{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siraj19/IPFSP/blob/main/ML_Course_Project_Scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_dAAKh5vlN6"
      },
      "source": [
        "**Instructions:** Just open it on Google Collab or as Juypter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your Excel file in Google Drive\n",
        "file_path = '/content/drive/My Drive/ICreatedManually/ML_Course_Project/OGDCL_DATASET_V4.xlsx'"
      ],
      "metadata": {
        "id": "-lAh4uxrB8Dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e61630-4d86-42f6-af01-dbd4b710b53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the excel file dataset\n",
        "def load_excel_file(file_path):\n",
        "    try:\n",
        "        # Load Excel file\n",
        "        xls = pd.ExcelFile(file_path)\n",
        "\n",
        "        # Load each sheet into a dictionary\n",
        "        data = {}\n",
        "        for sheet_name in xls.sheet_names:\n",
        "            data[sheet_name] = pd.read_excel(xls, sheet_name)\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error loading Excel file:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "M-cbJjVebt5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Excel file\n",
        "loaded_data = load_excel_file(file_path)"
      ],
      "metadata": {
        "id": "fyzRFRnQ70DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If data is loaded successfully, print it\n",
        "if loaded_data is not None:\n",
        "    for sheet_name, sheet_data in loaded_data.items():\n",
        "        print(f\"Sheet Name: {sheet_name}\")\n",
        "        print(sheet_data)"
      ],
      "metadata": {
        "id": "vsL489uF73eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c42dc8-b551-4d5d-d3fd-1ae4029fc6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheet Name: Yearly_Financial_Statement\n",
            "    Year         Equity         Assets   Liabilities  Current Assets  \\\n",
            "0   1999    33140000000    54489000000   21349000000     24133000000   \n",
            "1   2000    41499000000    59218000000   17719000000     29291000000   \n",
            "2   2001    51279000000    67762000000   16483000000     38121000000   \n",
            "3   2002    57239568000    74875595000   17636027000     36677661000   \n",
            "4   2003    64096779000    81815042000   17718263000     44039954000   \n",
            "5   2004    83190000000   104940000000   21750000000     57870000000   \n",
            "6   2005    90070000000   123280000000   33210000000     72710000000   \n",
            "7   2006   101470000000   129210000000   27740000000     76820000000   \n",
            "8   2007   106940000000   136750000000   29810000000     74920000000   \n",
            "9   2008   110420000000   152310000000   41890000000     79820000000   \n",
            "10  2009   126170000000   177990000000   51820000000     86830000000   \n",
            "11  2010   114380000000   228870000000  114490000000    120430000000   \n",
            "12  2011   158560000000   261780000000  103220000000    149600000000   \n",
            "13  2012   220370000000   338290000000  117920000000    214850000000   \n",
            "14  2013   269260000000   413930000000  144670000000    134250000000   \n",
            "15  2014   352660000000   496230000000  143570000000    194160000000   \n",
            "16  2015   399510000000   553790000000  154280000000    219780000000   \n",
            "17  2016   478632567000   589565539000  110932972000    254800670000   \n",
            "18  2017   512984337000   627287973000  114303636000    342460219000   \n",
            "19  2018   550556422000   666477197000  115920775000    405857905000   \n",
            "20  2019   646214995000   795058357000  148843362000    509465463000   \n",
            "21  2020   710563976000   888973636000  178409660000    555071602000   \n",
            "22  2021   769644045000   955993814000  186349769000    650669655000   \n",
            "23  2022   875392566000  1129983053000  254590487000    778537626000   \n",
            "24  2023  1082897877000  1424065294000  341167417000    959117636000   \n",
            "\n",
            "    Non-current Assets  Current Liabilities  Non Current Liabilities  \\\n",
            "0          30356000000          10814000000              10535000000   \n",
            "1          29927000000           7853000000               9866000000   \n",
            "2          29641000000          10970000000               5513000000   \n",
            "3           4091710000           7142303000              10493724000   \n",
            "4           3346624000           6455075000              11263188000   \n",
            "5          47070000000           4400000000              17360000000   \n",
            "6          50570000000          13590000000              19620000000   \n",
            "7          52390000000          11090000000              16650000000   \n",
            "8          61830000000          11260000000              18550000000   \n",
            "9          72490000000          21440000000              20460000000   \n",
            "10         91160000000          21290000000              30530000000   \n",
            "11        108440000000          34840000000              36630000000   \n",
            "12        112180000000          21780000000              38440000000   \n",
            "13        123440000000          32210000000              42690000000   \n",
            "14        279680000000          58380000000              43290000000   \n",
            "15        302070000000          48050000000              52520000000   \n",
            "16        334010000000          61900000000              49370000000   \n",
            "17        334764869000          58969148000              51963824000   \n",
            "18        284827754000          53610444000              60693192000   \n",
            "19        260619292000          55194887000              60725888000   \n",
            "20        285592894000          69902275000              78941087000   \n",
            "21        333902034000          89357746000              89051914000   \n",
            "22        305324159000         101679608000              84670161000   \n",
            "23        351445427000         139065523000             115524964000   \n",
            "24        464947658000         160964008000             180203409000   \n",
            "\n",
            "         ROE       ROA  Debt/Equity  Debt/Asset  SBP Policy Rate  \n",
            "0   0.137296  0.083503     0.644206    0.391804         0.141667  \n",
            "1   0.254464  0.178324     0.426974    0.299216         0.120000  \n",
            "2   0.321769  0.243499     0.321438    0.243248         0.125000  \n",
            "3   0.292979  0.223972     0.308109    0.235538         0.082500  \n",
            "4   0.322481  0.252643     0.276430    0.216565         0.075000  \n",
            "5   0.269383  0.213551     0.261450    0.207261         0.075000  \n",
            "6   0.366049  0.267440     0.368713    0.269387         0.090000  \n",
            "7   0.451365  0.354462     0.273381    0.214689         0.095000  \n",
            "8   0.423134  0.330896     0.278754    0.217989         0.100000  \n",
            "9   0.401558  0.291117     0.379370    0.275031         0.126250  \n",
            "10  0.440200  0.312040     0.410716    0.291140         0.131660  \n",
            "11  0.517398  0.258575     1.000962    0.500240         0.135000  \n",
            "12  0.400669  0.242685     0.650984    0.394301         0.127500  \n",
            "13  0.439760  0.286470     0.535100    0.348577         0.100000  \n",
            "14  0.338966  0.220496     0.537287    0.349504         0.095000  \n",
            "15  0.351358  0.249703     0.407106    0.289321         0.095000  \n",
            "16  0.218393  0.157551     0.386173    0.278589         0.075000  \n",
            "17  0.125294  0.101719     0.231771    0.188161         0.062500  \n",
            "18  0.124370  0.101708     0.222821    0.182219         0.062500  \n",
            "19  0.143019  0.118144     0.210552    0.173931         0.082000  \n",
            "20  0.183205  0.148907     0.230331    0.187211         0.128330  \n",
            "21  0.142056  0.113547     0.251082    0.200692         0.112000  \n",
            "22  0.118925  0.095743     0.242125    0.194928         0.095800  \n",
            "23  0.152823  0.118391     0.290830    0.225305         0.152500  \n",
            "24  0.207425  0.157732     0.315050    0.239573         0.210000  \n",
            "Sheet Name: Yearly_Profit_Loss_Statement\n",
            "    Year         Sales  Other Revenues  Earnings(Pre Tax)  Earnings(Post Tax)  \\\n",
            "0   1999   14230000000       600000000         4770000000          4550000000   \n",
            "1   2000   25300000000       910000000        12950000000         10560000000   \n",
            "2   2001   38300000000      1600000000        23230000000         16500000000   \n",
            "3   2002   39810000000      2040000000        25690000000         16770000000   \n",
            "4   2003   45070000000      1990000000        26420000000         20670000000   \n",
            "5   2004   51330000000      1310000000        30520000000         22410000000   \n",
            "6   2005   73710000000      2280000000        49020000000         32970000000   \n",
            "7   2006   97310000000      4400000000        65760000000         45800000000   \n",
            "8   2007  100730000000      4030000000        60750000000         45250000000   \n",
            "9   2008  125910000000      3910000000        78310000000         44340000000   \n",
            "10  2009  130830000000      3430000000        80930000000         55540000000   \n",
            "11  2010  142570000000      3360000000        88550000000         59180000000   \n",
            "12  2011  155630000000      3380000000        90980000000         63530000000   \n",
            "13  2012  197840000000      9750000000       133080000000         96910000000   \n",
            "14  2013  223370000000     15800000000       146810000000         91270000000   \n",
            "15  2014  257010000000     19240000000       172350000000        123910000000   \n",
            "16  2015  210620000000     20230000000       127030000000         87250000000   \n",
            "17  2016  162870000000     16890000000        80510000000         59970000000   \n",
            "18  2017  171830000000     17850000000        89140000000         63800000000   \n",
            "19  2018  205340000000     19080000000       112630000000         78740000000   \n",
            "20  2019  261480000000     37150000000       176600000000        118390000000   \n",
            "21  2020  232930000000     39880000000       144360000000        100940000000   \n",
            "22  2021  239100000000     20270000000       128990000000         91530000000   \n",
            "23  2022  335460000000     50680000000       232520000000        133780000000   \n",
            "24  2023  413590000000    165240000000       383770000000        224620000000   \n",
            "\n",
            "    Crude Oil (Thousand Barrels)  Gas (MMcf)  LPG (Tons)  Sulphur (Tons)  \\\n",
            "0                           8074      115967       90425           29880   \n",
            "1                           8907      161534       93004           13445   \n",
            "2                           8535      217927       77402           16670   \n",
            "3                           8705      245537       93136           23234   \n",
            "4                           9413      274006       90304           15889   \n",
            "5                           9941      277408      101322           18917   \n",
            "6                          13045      329385      120063           25884   \n",
            "7                          12956      344164      128654           22006   \n",
            "8                          13930      344032      139480           16638   \n",
            "9                          15037      358868      125482           29065   \n",
            "10                         14438      364036       79145           24673   \n",
            "11                         13343      354327       73881           20018   \n",
            "12                         13224      362924       71061           34400   \n",
            "13                         13713      381863       75005           21400   \n",
            "14                         14183      392513       41003           14493   \n",
            "15                         14734      416238       64088           27707   \n",
            "16                         14591      404128       95629           23600   \n",
            "17                         14461      386637      125241           15800   \n",
            "18                         15744      383692      164407           23800   \n",
            "19                         14867      373192      250984           24800   \n",
            "20                         14555      370217      294167           20900   \n",
            "21                         12919      326879      269806           19000   \n",
            "22                         13230      317443      293310           24000   \n",
            "23                         12528      301286      294619           15800   \n",
            "24                         11432      278903      261798            7200   \n",
            "\n",
            "    Payouts/Dividends  Payouts/Dividends Ratio  USD to PKR  \n",
            "0          1075000000                 0.236264   51.367858  \n",
            "1          2150000000                 0.203598   53.875788  \n",
            "2          6666000000                 0.404000   61.676154  \n",
            "3         10752000000                 0.641145   59.543115  \n",
            "4         12903000000                 0.624238   57.703349  \n",
            "5         17200000000                 0.767515   58.351176  \n",
            "6         32260000000                 0.978465   59.603242  \n",
            "7         38710000000                 0.845197   60.306531  \n",
            "8         38710000000                 0.855470   60.773820  \n",
            "9         40792800000                 0.920000   70.597996  \n",
            "10        35545600000                 0.640000   81.563582  \n",
            "11        23672000000                 0.400000   85.197625  \n",
            "12        23506100000                 0.370000   86.417729  \n",
            "13        31011200000                 0.320000   93.410351  \n",
            "14        35595300000                 0.390000  101.581773  \n",
            "15        39651200000                 0.320000  101.008914  \n",
            "16        33155000000                 0.380000  102.782492  \n",
            "17        22188900000                 0.370000  104.720974  \n",
            "18        25520000000                 0.400000  105.341074  \n",
            "19        43307000000                 0.550000  121.370905  \n",
            "20        47356000000                 0.400000  150.104092  \n",
            "21        29272600000                 0.290000  161.128903  \n",
            "22        29289600000                 0.320000  162.393796  \n",
            "23        30769400000                 0.230000  204.468923  \n",
            "24        35939200000                 0.160000  279.832425  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Assuming loaded_data is a dictionary where keys are sheet names and values are Pandas DataFrames\n",
        "# Assuming features_columns contains the names of the columns to be used as features\n",
        "# Assuming label_column contains the name of the column to be used as label\n",
        "\n",
        "# 1. Select the desired columns from your DataFrame\n",
        "sheet_name1 = 'Yearly_Profit_Loss_Statement'\n",
        "sheet_name2 = 'Yearly_Financial_Statement'\n",
        "\n",
        "features_columns1 = ['Sales', 'Other Revenues', 'Earnings(Pre Tax)', 'Earnings(Post Tax)', 'Crude Oil (Thousand Barrels)', 'Gas (MMcf)', 'Payouts/Dividends', 'Payouts/Dividends Ratio', 'USD to PKR']\n",
        "features_columns2 = ['Equity', 'Assets', 'Liabilities', 'Current Assets', 'Non-current Assets', 'Current Liabilities', 'Non Current Liabilities', 'ROE', 'ROA', 'Debt/Equity', 'Debt/Asset', 'SBP Policy Rate']\n",
        "\n",
        "\n",
        "# Get the DataFrame for the specified sheet\n",
        "df1 = loaded_data[sheet_name1]\n",
        "df2 = loaded_data[sheet_name2]\n",
        "\n",
        "# Select features\n",
        "features1 = df1[features_columns1]\n",
        "features2 = df2[features_columns2]\n",
        "\n",
        "# Concatenate features along the appropriate axis\n",
        "features = pd.concat([features1, features2], axis=1)\n",
        "\n",
        "\n",
        "# Calculate the mean for each feature\n",
        "maxs = features.max()\n",
        "\n",
        "features_scale_factor = np.array(maxs)\n",
        "normalized_features = features.div(maxs)"
      ],
      "metadata": {
        "id": "8izr-ZF0egaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalized_features)"
      ],
      "metadata": {
        "id": "tCZstcOqi8HI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9feccf2f-7ffe-4fea-f5b9-91a3b105f041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Sales  Other Revenues  Earnings(Pre Tax)  Earnings(Post Tax)  \\\n",
            "0   0.034406        0.003631           0.012429            0.020256   \n",
            "1   0.061172        0.005507           0.033744            0.047013   \n",
            "2   0.092604        0.009683           0.060531            0.073457   \n",
            "3   0.096255        0.012346           0.066941            0.074659   \n",
            "4   0.108973        0.012043           0.068843            0.092022   \n",
            "5   0.124108        0.007928           0.079527            0.099768   \n",
            "6   0.178220        0.013798           0.127733            0.146781   \n",
            "7   0.235281        0.026628           0.171353            0.203900   \n",
            "8   0.243550        0.024389           0.158298            0.201451   \n",
            "9   0.304432        0.023663           0.204055            0.197400   \n",
            "10  0.316328        0.020758           0.210882            0.247262   \n",
            "11  0.344713        0.020334           0.230737            0.263467   \n",
            "12  0.376291        0.020455           0.237069            0.282833   \n",
            "13  0.478348        0.059005           0.346770            0.431440   \n",
            "14  0.540076        0.095618           0.382547            0.406331   \n",
            "15  0.621413        0.116437           0.449097            0.551643   \n",
            "16  0.509248        0.122428           0.331006            0.388434   \n",
            "17  0.393796        0.102215           0.209787            0.266984   \n",
            "18  0.415460        0.108025           0.232275            0.284035   \n",
            "19  0.496482        0.115468           0.293483            0.350548   \n",
            "20  0.632220        0.224824           0.460171            0.527068   \n",
            "21  0.563191        0.241346           0.376163            0.449381   \n",
            "22  0.578109        0.122670           0.336113            0.407488   \n",
            "23  0.811093        0.306705           0.605884            0.595584   \n",
            "24  1.000000        1.000000           1.000000            1.000000   \n",
            "\n",
            "    Crude Oil (Thousand Barrels)  Gas (MMcf)  Payouts/Dividends  \\\n",
            "0                       0.512830    0.278607           0.022700   \n",
            "1                       0.565739    0.388081           0.045401   \n",
            "2                       0.542111    0.523563           0.140764   \n",
            "3                       0.552909    0.589896           0.227046   \n",
            "4                       0.597879    0.658292           0.272468   \n",
            "5                       0.631415    0.666465           0.363206   \n",
            "6                       0.828570    0.791338           0.681223   \n",
            "7                       0.822917    0.826844           0.817425   \n",
            "8                       0.884782    0.826527           0.817425   \n",
            "9                       0.955094    0.862170           0.861407   \n",
            "10                      0.917048    0.874586           0.750604   \n",
            "11                      0.847497    0.851261           0.499873   \n",
            "12                      0.839939    0.871915           0.496370   \n",
            "13                      0.870998    0.917415           0.654853   \n",
            "14                      0.900851    0.943001           0.751653   \n",
            "15                      0.935849    1.000000           0.837300   \n",
            "16                      0.926766    0.970906           0.700122   \n",
            "17                      0.918509    0.928884           0.468555   \n",
            "18                      1.000000    0.921809           0.538897   \n",
            "19                      0.944296    0.896583           0.914499   \n",
            "20                      0.924479    0.889436           1.000000   \n",
            "21                      0.820567    0.785318           0.618139   \n",
            "22                      0.840320    0.762648           0.618498   \n",
            "23                      0.795732    0.723831           0.649747   \n",
            "24                      0.726118    0.670057           0.758915   \n",
            "\n",
            "    Payouts/Dividends Ratio  USD to PKR    Equity  ...  Liabilities  \\\n",
            "0                  0.241464    0.183566  0.030603  ...     0.062576   \n",
            "1                  0.208079    0.192529  0.038322  ...     0.051936   \n",
            "2                  0.412892    0.220404  0.047353  ...     0.048314   \n",
            "3                  0.655256    0.212781  0.052858  ...     0.051693   \n",
            "4                  0.637977    0.206207  0.059190  ...     0.051934   \n",
            "5                  0.784406    0.208522  0.076822  ...     0.063752   \n",
            "6                  1.000000    0.212996  0.083175  ...     0.097342   \n",
            "7                  0.863798    0.215509  0.093702  ...     0.081309   \n",
            "8                  0.874297    0.217179  0.098754  ...     0.087376   \n",
            "9                  0.940248    0.252287  0.101967  ...     0.122784   \n",
            "10                 0.654086    0.291473  0.116511  ...     0.151890   \n",
            "11                 0.408803    0.304459  0.105624  ...     0.335583   \n",
            "12                 0.378143    0.308820  0.146422  ...     0.302549   \n",
            "13                 0.327043    0.333808  0.203500  ...     0.345637   \n",
            "14                 0.398583    0.363009  0.248648  ...     0.424044   \n",
            "15                 0.327043    0.360962  0.325663  ...     0.420820   \n",
            "16                 0.388363    0.367300  0.368927  ...     0.452212   \n",
            "17                 0.378143    0.374227  0.441992  ...     0.325157   \n",
            "18                 0.408803    0.376443  0.473714  ...     0.335037   \n",
            "19                 0.562105    0.433727  0.508410  ...     0.339777   \n",
            "20                 0.408803    0.536407  0.596746  ...     0.436277   \n",
            "21                 0.296383    0.575805  0.656169  ...     0.522939   \n",
            "22                 0.327043    0.580325  0.710726  ...     0.546212   \n",
            "23                 0.235062    0.730683  0.808380  ...     0.746233   \n",
            "24                 0.163521    1.000000  1.000000  ...     1.000000   \n",
            "\n",
            "    Current Assets  Non-current Assets  Current Liabilities  \\\n",
            "0         0.025162            0.065289             0.067183   \n",
            "1         0.030540            0.064366             0.048787   \n",
            "2         0.039746            0.063751             0.068152   \n",
            "3         0.038241            0.008800             0.044372   \n",
            "4         0.045917            0.007198             0.040103   \n",
            "5         0.060337            0.101237             0.027335   \n",
            "6         0.075809            0.108765             0.084429   \n",
            "7         0.080094            0.112679             0.068897   \n",
            "8         0.078113            0.132983             0.069954   \n",
            "9         0.083222            0.155910             0.133197   \n",
            "10        0.090531            0.196065             0.132266   \n",
            "11        0.125563            0.233231             0.216446   \n",
            "12        0.155977            0.241274             0.135310   \n",
            "13        0.224008            0.265492             0.200107   \n",
            "14        0.139972            0.601530             0.362690   \n",
            "15        0.202436            0.649686             0.298514   \n",
            "16        0.229148            0.718382             0.384558   \n",
            "17        0.265662            0.720005             0.366350   \n",
            "18        0.357058            0.612602             0.333059   \n",
            "19        0.423158            0.560535             0.342902   \n",
            "20        0.531181            0.614247             0.434273   \n",
            "21        0.578732            0.718150             0.555141   \n",
            "22        0.678404            0.656685             0.631692   \n",
            "23        0.811723            0.755882             0.863954   \n",
            "24        1.000000            1.000000             1.000000   \n",
            "\n",
            "    Non Current Liabilities       ROE       ROA  Debt/Equity  Debt/Asset  \\\n",
            "0                  0.058462  0.265359  0.235577     0.643587    0.783231   \n",
            "1                  0.054749  0.491815  0.503084     0.426564    0.598145   \n",
            "2                  0.030593  0.621899  0.686955     0.321129    0.486263   \n",
            "3                  0.058233  0.566255  0.631864     0.307813    0.470849   \n",
            "4                  0.062503  0.623275  0.712751     0.276164    0.432922   \n",
            "5                  0.096336  0.520650  0.602464     0.261198    0.414323   \n",
            "6                  0.108877  0.707480  0.754496     0.368359    0.538515   \n",
            "7                  0.092396  0.872374  1.000000     0.273119    0.429172   \n",
            "8                  0.102939  0.817812  0.933516     0.278487    0.435769   \n",
            "9                  0.113538  0.776110  0.821293     0.379005    0.549798   \n",
            "10                 0.169420  0.850795  0.880321     0.410321    0.582000   \n",
            "11                 0.203270  1.000000  0.729486     1.000000    1.000000   \n",
            "12                 0.213314  0.774391  0.684657     0.650358    0.788222   \n",
            "13                 0.236899  0.849946  0.808184     0.534586    0.696818   \n",
            "14                 0.240229  0.655136  0.622059     0.536771    0.698671   \n",
            "15                 0.291448  0.679087  0.704456     0.406715    0.578365   \n",
            "16                 0.273968  0.422098  0.444479     0.385802    0.556911   \n",
            "17                 0.288362  0.242163  0.286967     0.231548    0.376140   \n",
            "18                 0.336804  0.240376  0.286936     0.222607    0.364262   \n",
            "19                 0.336985  0.276420  0.333304     0.210350    0.347694   \n",
            "20                 0.438067  0.354090  0.420094     0.230110    0.374241   \n",
            "21                 0.494174  0.274559  0.320335     0.250841    0.401191   \n",
            "22                 0.469859  0.229852  0.270109     0.241892    0.389668   \n",
            "23                 0.641081  0.295368  0.334003     0.290551    0.450393   \n",
            "24                 1.000000  0.400900  0.444989     0.314748    0.478916   \n",
            "\n",
            "    SBP Policy Rate  \n",
            "0          0.674603  \n",
            "1          0.571429  \n",
            "2          0.595238  \n",
            "3          0.392857  \n",
            "4          0.357143  \n",
            "5          0.357143  \n",
            "6          0.428571  \n",
            "7          0.452381  \n",
            "8          0.476190  \n",
            "9          0.601190  \n",
            "10         0.626952  \n",
            "11         0.642857  \n",
            "12         0.607143  \n",
            "13         0.476190  \n",
            "14         0.452381  \n",
            "15         0.452381  \n",
            "16         0.357143  \n",
            "17         0.297619  \n",
            "18         0.297619  \n",
            "19         0.390476  \n",
            "20         0.611095  \n",
            "21         0.533333  \n",
            "22         0.456190  \n",
            "23         0.726190  \n",
            "24         1.000000  \n",
            "\n",
            "[25 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = normalized_features.values"
      ],
      "metadata": {
        "id": "H6IsVBEdRs5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define window size and step size\n",
        "window_size = 3  # Number of past observations to consider\n",
        "step_size = 1     # Step size for sliding window\n",
        "\n",
        "# Prepare input sequences and target values\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data) - window_size - 1):\n",
        "    X.append(data[i:(i + window_size)])\n",
        "    y.append(data[i + window_size])\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "lGEd0DYANJIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "HEHvAcAaNuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "vkkaEPTQ1YHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b9d286-610d-43d8-cd4b-c704078cb428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.49648202 0.11546841 0.29348308 0.35054759 0.94429624 0.8965832\n",
            "  0.91449869 0.56210477 0.4337271  0.50841029 0.46801028 0.33977681\n",
            "  0.42315759 0.56053469 0.34290204 0.33698523 0.27641952 0.33330418\n",
            "  0.21034974 0.34769408 0.39047619]\n",
            " [0.63222031 0.2248245  0.46017146 0.52706794 0.92447917 0.88943585\n",
            "  1.         0.40880347 0.53640707 0.59674602 0.5583019  0.4362766\n",
            "  0.53118141 0.61424741 0.4342727  0.43806656 0.35408956 0.42009418\n",
            "  0.23010973 0.37424136 0.61109524]\n",
            " [0.5631906  0.24134592 0.37616281 0.44938118 0.82056657 0.78531753\n",
            "  0.6181392  0.29638252 0.57580498 0.65616896 0.62425062 0.52293874\n",
            "  0.57873151 0.71814973 0.55514116 0.49417441 0.27455872 0.3203355\n",
            "  0.25084053 0.40119065 0.53333333]\n",
            " [0.57810876 0.12267006 0.33611278 0.4074882  0.84032012 0.76264781\n",
            "  0.61849818 0.32704278 0.58032516 0.71072634 0.67131319 0.54621209\n",
            "  0.67840443 0.65668501 0.63169158 0.46985882 0.22985221 0.27010898\n",
            "  0.24189199 0.38966831 0.45619048]\n",
            " [0.81109311 0.3067054  0.60588373 0.59558365 0.79573171 0.72383108\n",
            "  0.6497466  0.235062   0.73068345 0.80837961 0.79349104 0.7462333\n",
            "  0.81172277 0.7558817  0.86395415 0.6410809  0.29536802 0.33400265\n",
            "  0.29055065 0.45039292 0.72619048]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=500, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "wAsZ9m_IXu95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "#from keras.layers import Activation\n",
        "# Define the RNN model with an additional Dense layer\n",
        "model = Sequential([\n",
        "    LSTM(units=64, input_shape=(window_size, len(features_columns1)+len(features_columns2))),\n",
        "    Dense(units=32, activation='relu'),  # Additional Dense layer\n",
        "    Dense(units=32, activation='relu'),  # Additional Dense layer\n",
        "    Dense(units=len(features_columns1)+len(features_columns2)),  # Output layer with the same number of features\n",
        "])\n"
      ],
      "metadata": {
        "id": "kRPdP9vHNwXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f70Z8tBhFt32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "elfbvGUPNwfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, epochs=1500, batch_size=15, validation_split=0.3, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "aQ7aBbedNwi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc63c1d-92a8-408b-e678-74f883426570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1500\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.2242 - val_loss: 0.2471\n",
            "Epoch 2/1500\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.2178 - val_loss: 0.2387\n",
            "Epoch 3/1500\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2118 - val_loss: 0.2309\n",
            "Epoch 4/1500\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.2063 - val_loss: 0.2236\n",
            "Epoch 5/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.2015 - val_loss: 0.2174\n",
            "Epoch 6/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.1970 - val_loss: 0.2114\n",
            "Epoch 7/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1924 - val_loss: 0.2054\n",
            "Epoch 8/1500\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.1876 - val_loss: 0.1993\n",
            "Epoch 9/1500\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.1828 - val_loss: 0.1930\n",
            "Epoch 10/1500\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.1779 - val_loss: 0.1865\n",
            "Epoch 11/1500\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.1729 - val_loss: 0.1798\n",
            "Epoch 12/1500\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.1678 - val_loss: 0.1730\n",
            "Epoch 13/1500\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.1627 - val_loss: 0.1663\n",
            "Epoch 14/1500\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.1575 - val_loss: 0.1598\n",
            "Epoch 15/1500\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.1524 - val_loss: 0.1533\n",
            "Epoch 16/1500\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.1474 - val_loss: 0.1473\n",
            "Epoch 17/1500\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1424 - val_loss: 0.1415\n",
            "Epoch 18/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.1376 - val_loss: 0.1362\n",
            "Epoch 19/1500\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.1329 - val_loss: 0.1311\n",
            "Epoch 20/1500\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1283 - val_loss: 0.1264\n",
            "Epoch 21/1500\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1238 - val_loss: 0.1219\n",
            "Epoch 22/1500\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1195 - val_loss: 0.1175\n",
            "Epoch 23/1500\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.1151 - val_loss: 0.1130\n",
            "Epoch 24/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.1108 - val_loss: 0.1085\n",
            "Epoch 25/1500\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.1065 - val_loss: 0.1039\n",
            "Epoch 26/1500\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.1021 - val_loss: 0.0993\n",
            "Epoch 27/1500\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0978 - val_loss: 0.0952\n",
            "Epoch 28/1500\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0938 - val_loss: 0.0915\n",
            "Epoch 29/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0899 - val_loss: 0.0880\n",
            "Epoch 30/1500\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0860 - val_loss: 0.0847\n",
            "Epoch 31/1500\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0822 - val_loss: 0.0815\n",
            "Epoch 32/1500\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0786 - val_loss: 0.0787\n",
            "Epoch 33/1500\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0751 - val_loss: 0.0763\n",
            "Epoch 34/1500\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0718 - val_loss: 0.0742\n",
            "Epoch 35/1500\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0687 - val_loss: 0.0723\n",
            "Epoch 36/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0658 - val_loss: 0.0705\n",
            "Epoch 37/1500\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.0629 - val_loss: 0.0688\n",
            "Epoch 38/1500\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.0602 - val_loss: 0.0671\n",
            "Epoch 39/1500\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.0573 - val_loss: 0.0654\n",
            "Epoch 40/1500\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0543 - val_loss: 0.0638\n",
            "Epoch 41/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0514 - val_loss: 0.0623\n",
            "Epoch 42/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0486 - val_loss: 0.0610\n",
            "Epoch 43/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0457 - val_loss: 0.0599\n",
            "Epoch 44/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0430 - val_loss: 0.0589\n",
            "Epoch 45/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0403 - val_loss: 0.0582\n",
            "Epoch 46/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0377 - val_loss: 0.0575\n",
            "Epoch 47/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0352 - val_loss: 0.0570\n",
            "Epoch 48/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0329 - val_loss: 0.0566\n",
            "Epoch 49/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0306 - val_loss: 0.0563\n",
            "Epoch 50/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0285 - val_loss: 0.0561\n",
            "Epoch 51/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0264 - val_loss: 0.0560\n",
            "Epoch 52/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0245 - val_loss: 0.0560\n",
            "Epoch 53/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0227 - val_loss: 0.0564\n",
            "Epoch 54/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0211 - val_loss: 0.0569\n",
            "Epoch 55/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0196 - val_loss: 0.0576\n",
            "Epoch 56/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0182 - val_loss: 0.0585\n",
            "Epoch 57/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0171 - val_loss: 0.0595\n",
            "Epoch 58/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0161 - val_loss: 0.0605\n",
            "Epoch 59/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0152 - val_loss: 0.0616\n",
            "Epoch 60/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0144 - val_loss: 0.0627\n",
            "Epoch 61/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0138 - val_loss: 0.0636\n",
            "Epoch 62/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0134 - val_loss: 0.0643\n",
            "Epoch 63/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0130 - val_loss: 0.0648\n",
            "Epoch 64/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0126 - val_loss: 0.0650\n",
            "Epoch 65/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0124 - val_loss: 0.0650\n",
            "Epoch 66/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - val_loss: 0.0649\n",
            "Epoch 67/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0120 - val_loss: 0.0646\n",
            "Epoch 68/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0118 - val_loss: 0.0643\n",
            "Epoch 69/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0117 - val_loss: 0.0640\n",
            "Epoch 70/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0116 - val_loss: 0.0638\n",
            "Epoch 71/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0115 - val_loss: 0.0636\n",
            "Epoch 72/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0114 - val_loss: 0.0633\n",
            "Epoch 73/1500\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0113 - val_loss: 0.0632\n",
            "Epoch 74/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0112 - val_loss: 0.0630\n",
            "Epoch 75/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0111 - val_loss: 0.0628\n",
            "Epoch 76/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0110 - val_loss: 0.0625\n",
            "Epoch 77/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0109 - val_loss: 0.0622\n",
            "Epoch 78/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0108 - val_loss: 0.0619\n",
            "Epoch 79/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0108 - val_loss: 0.0615\n",
            "Epoch 80/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0107 - val_loss: 0.0611\n",
            "Epoch 81/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0107 - val_loss: 0.0606\n",
            "Epoch 82/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0106 - val_loss: 0.0602\n",
            "Epoch 83/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0106 - val_loss: 0.0598\n",
            "Epoch 84/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0106 - val_loss: 0.0594\n",
            "Epoch 85/1500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0105 - val_loss: 0.0592\n",
            "Epoch 86/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0105 - val_loss: 0.0589\n",
            "Epoch 87/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0105 - val_loss: 0.0588\n",
            "Epoch 88/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0105 - val_loss: 0.0586\n",
            "Epoch 89/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0105 - val_loss: 0.0585\n",
            "Epoch 90/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0104 - val_loss: 0.0584\n",
            "Epoch 91/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0104 - val_loss: 0.0582\n",
            "Epoch 92/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0104 - val_loss: 0.0581\n",
            "Epoch 93/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0104 - val_loss: 0.0579\n",
            "Epoch 94/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0103 - val_loss: 0.0577\n",
            "Epoch 95/1500\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0103 - val_loss: 0.0574\n",
            "Epoch 96/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0103 - val_loss: 0.0572\n",
            "Epoch 97/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0103 - val_loss: 0.0569\n",
            "Epoch 98/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0102 - val_loss: 0.0567\n",
            "Epoch 99/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0102 - val_loss: 0.0565\n",
            "Epoch 100/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0102 - val_loss: 0.0563\n",
            "Epoch 101/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0101 - val_loss: 0.0561\n",
            "Epoch 102/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0101 - val_loss: 0.0559\n",
            "Epoch 103/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0101 - val_loss: 0.0558\n",
            "Epoch 104/1500\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0101 - val_loss: 0.0557\n",
            "Epoch 105/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0100 - val_loss: 0.0555\n",
            "Epoch 106/1500\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0100 - val_loss: 0.0554\n",
            "Epoch 107/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0100 - val_loss: 0.0553\n",
            "Epoch 108/1500\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0100 - val_loss: 0.0551\n",
            "Epoch 109/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0099 - val_loss: 0.0550\n",
            "Epoch 110/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0099 - val_loss: 0.0548\n",
            "Epoch 111/1500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0099 - val_loss: 0.0546\n",
            "Epoch 112/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0099 - val_loss: 0.0545\n",
            "Epoch 113/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0098 - val_loss: 0.0543\n",
            "Epoch 114/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0098 - val_loss: 0.0541\n",
            "Epoch 115/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0098 - val_loss: 0.0539\n",
            "Epoch 116/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0098 - val_loss: 0.0537\n",
            "Epoch 117/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0097 - val_loss: 0.0535\n",
            "Epoch 118/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0097 - val_loss: 0.0533\n",
            "Epoch 119/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0097 - val_loss: 0.0531\n",
            "Epoch 120/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0097 - val_loss: 0.0529\n",
            "Epoch 121/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0527\n",
            "Epoch 122/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0525\n",
            "Epoch 123/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0523\n",
            "Epoch 124/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0096 - val_loss: 0.0521\n",
            "Epoch 125/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0095 - val_loss: 0.0518\n",
            "Epoch 126/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0095 - val_loss: 0.0516\n",
            "Epoch 127/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0095 - val_loss: 0.0514\n",
            "Epoch 128/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0094 - val_loss: 0.0512\n",
            "Epoch 129/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0094 - val_loss: 0.0510\n",
            "Epoch 130/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0094 - val_loss: 0.0508\n",
            "Epoch 131/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0093 - val_loss: 0.0506\n",
            "Epoch 132/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0093 - val_loss: 0.0504\n",
            "Epoch 133/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0502\n",
            "Epoch 134/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0092 - val_loss: 0.0500\n",
            "Epoch 135/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0092 - val_loss: 0.0498\n",
            "Epoch 136/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0495\n",
            "Epoch 137/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0091 - val_loss: 0.0493\n",
            "Epoch 138/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0091 - val_loss: 0.0490\n",
            "Epoch 139/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0091 - val_loss: 0.0487\n",
            "Epoch 140/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0090 - val_loss: 0.0484\n",
            "Epoch 141/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0090 - val_loss: 0.0481\n",
            "Epoch 142/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0089 - val_loss: 0.0478\n",
            "Epoch 143/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0089 - val_loss: 0.0475\n",
            "Epoch 144/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0089 - val_loss: 0.0473\n",
            "Epoch 145/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0088 - val_loss: 0.0470\n",
            "Epoch 146/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0088 - val_loss: 0.0468\n",
            "Epoch 147/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0087 - val_loss: 0.0466\n",
            "Epoch 148/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0087 - val_loss: 0.0464\n",
            "Epoch 149/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0087 - val_loss: 0.0462\n",
            "Epoch 150/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0086 - val_loss: 0.0459\n",
            "Epoch 151/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0086 - val_loss: 0.0457\n",
            "Epoch 152/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0085 - val_loss: 0.0454\n",
            "Epoch 153/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0085 - val_loss: 0.0451\n",
            "Epoch 154/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0084 - val_loss: 0.0448\n",
            "Epoch 155/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0084 - val_loss: 0.0445\n",
            "Epoch 156/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0083 - val_loss: 0.0442\n",
            "Epoch 157/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0083 - val_loss: 0.0439\n",
            "Epoch 158/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0082 - val_loss: 0.0437\n",
            "Epoch 159/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0082 - val_loss: 0.0435\n",
            "Epoch 160/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0081 - val_loss: 0.0433\n",
            "Epoch 161/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0080 - val_loss: 0.0432\n",
            "Epoch 162/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0080 - val_loss: 0.0431\n",
            "Epoch 163/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0079 - val_loss: 0.0430\n",
            "Epoch 164/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0079 - val_loss: 0.0429\n",
            "Epoch 165/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0078 - val_loss: 0.0427\n",
            "Epoch 166/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0077 - val_loss: 0.0425\n",
            "Epoch 167/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0077 - val_loss: 0.0423\n",
            "Epoch 168/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0076 - val_loss: 0.0420\n",
            "Epoch 169/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0075 - val_loss: 0.0417\n",
            "Epoch 170/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0075 - val_loss: 0.0414\n",
            "Epoch 171/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0074 - val_loss: 0.0410\n",
            "Epoch 172/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0073 - val_loss: 0.0407\n",
            "Epoch 173/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0072 - val_loss: 0.0404\n",
            "Epoch 174/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0072 - val_loss: 0.0401\n",
            "Epoch 175/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0071 - val_loss: 0.0397\n",
            "Epoch 176/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0070 - val_loss: 0.0395\n",
            "Epoch 177/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0069 - val_loss: 0.0392\n",
            "Epoch 178/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0069 - val_loss: 0.0390\n",
            "Epoch 179/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0068 - val_loss: 0.0388\n",
            "Epoch 180/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0067 - val_loss: 0.0386\n",
            "Epoch 181/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0067 - val_loss: 0.0383\n",
            "Epoch 182/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0066 - val_loss: 0.0380\n",
            "Epoch 183/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0065 - val_loss: 0.0378\n",
            "Epoch 184/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0065 - val_loss: 0.0376\n",
            "Epoch 185/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0064 - val_loss: 0.0374\n",
            "Epoch 186/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0063 - val_loss: 0.0371\n",
            "Epoch 187/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0063 - val_loss: 0.0369\n",
            "Epoch 188/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0062 - val_loss: 0.0367\n",
            "Epoch 189/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0062 - val_loss: 0.0365\n",
            "Epoch 190/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0061 - val_loss: 0.0363\n",
            "Epoch 191/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0060 - val_loss: 0.0362\n",
            "Epoch 192/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0060 - val_loss: 0.0360\n",
            "Epoch 193/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0059 - val_loss: 0.0358\n",
            "Epoch 194/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0059 - val_loss: 0.0355\n",
            "Epoch 195/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0058 - val_loss: 0.0352\n",
            "Epoch 196/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0057 - val_loss: 0.0350\n",
            "Epoch 197/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0057 - val_loss: 0.0349\n",
            "Epoch 198/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0056 - val_loss: 0.0347\n",
            "Epoch 199/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0055 - val_loss: 0.0346\n",
            "Epoch 200/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0055 - val_loss: 0.0344\n",
            "Epoch 201/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0054 - val_loss: 0.0343\n",
            "Epoch 202/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0054 - val_loss: 0.0343\n",
            "Epoch 203/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0053 - val_loss: 0.0344\n",
            "Epoch 204/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0052 - val_loss: 0.0345\n",
            "Epoch 205/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0052 - val_loss: 0.0347\n",
            "Epoch 206/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0350\n",
            "Epoch 207/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0051 - val_loss: 0.0353\n",
            "Epoch 208/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0050 - val_loss: 0.0355\n",
            "Epoch 209/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0049 - val_loss: 0.0357\n",
            "Epoch 210/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0049 - val_loss: 0.0358\n",
            "Epoch 211/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0048 - val_loss: 0.0359\n",
            "Epoch 212/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0048 - val_loss: 0.0360\n",
            "Epoch 213/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0047 - val_loss: 0.0362\n",
            "Epoch 214/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0047 - val_loss: 0.0365\n",
            "Epoch 215/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0046 - val_loss: 0.0367\n",
            "Epoch 216/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0046 - val_loss: 0.0369\n",
            "Epoch 217/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0045 - val_loss: 0.0371\n",
            "Epoch 218/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0045 - val_loss: 0.0373\n",
            "Epoch 219/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0044 - val_loss: 0.0373\n",
            "Epoch 220/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0044 - val_loss: 0.0373\n",
            "Epoch 221/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0043 - val_loss: 0.0373\n",
            "Epoch 222/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0043 - val_loss: 0.0374\n",
            "Epoch 223/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0042 - val_loss: 0.0375\n",
            "Epoch 224/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0042 - val_loss: 0.0376\n",
            "Epoch 225/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0041 - val_loss: 0.0377\n",
            "Epoch 226/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0041 - val_loss: 0.0378\n",
            "Epoch 227/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0040 - val_loss: 0.0378\n",
            "Epoch 228/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0040 - val_loss: 0.0378\n",
            "Epoch 229/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0040 - val_loss: 0.0379\n",
            "Epoch 230/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0039 - val_loss: 0.0380\n",
            "Epoch 231/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0039 - val_loss: 0.0381\n",
            "Epoch 232/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0038 - val_loss: 0.0383\n",
            "Epoch 233/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0038 - val_loss: 0.0384\n",
            "Epoch 234/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0037 - val_loss: 0.0385\n",
            "Epoch 235/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0037 - val_loss: 0.0385\n",
            "Epoch 236/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0037 - val_loss: 0.0385\n",
            "Epoch 237/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0036 - val_loss: 0.0385\n",
            "Epoch 238/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0036 - val_loss: 0.0386\n",
            "Epoch 239/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0036 - val_loss: 0.0387\n",
            "Epoch 240/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0035 - val_loss: 0.0388\n",
            "Epoch 241/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0035 - val_loss: 0.0390\n",
            "Epoch 242/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0035 - val_loss: 0.0391\n",
            "Epoch 243/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0034 - val_loss: 0.0392\n",
            "Epoch 244/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0034 - val_loss: 0.0393\n",
            "Epoch 245/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0394\n",
            "Epoch 246/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0033 - val_loss: 0.0395\n",
            "Epoch 247/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0033 - val_loss: 0.0396\n",
            "Epoch 248/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0033 - val_loss: 0.0397\n",
            "Epoch 249/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0032 - val_loss: 0.0399\n",
            "Epoch 250/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0032 - val_loss: 0.0401\n",
            "Epoch 251/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0032 - val_loss: 0.0403\n",
            "Epoch 252/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0031 - val_loss: 0.0403\n",
            "Epoch 253/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0404\n",
            "Epoch 254/1500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0406\n",
            "Epoch 255/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0031 - val_loss: 0.0407\n",
            "Epoch 256/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0030 - val_loss: 0.0408\n",
            "Epoch 257/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0030 - val_loss: 0.0409\n",
            "Epoch 258/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0030 - val_loss: 0.0411\n",
            "Epoch 259/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0029 - val_loss: 0.0412\n",
            "Epoch 260/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0029 - val_loss: 0.0413\n",
            "Epoch 261/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0029 - val_loss: 0.0416\n",
            "Epoch 262/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0029 - val_loss: 0.0417\n",
            "Epoch 263/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0029 - val_loss: 0.0418\n",
            "Epoch 264/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0028 - val_loss: 0.0419\n",
            "Epoch 265/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0028 - val_loss: 0.0421\n",
            "Epoch 266/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0028 - val_loss: 0.0422\n",
            "Epoch 267/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0027 - val_loss: 0.0424\n",
            "Epoch 268/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0027 - val_loss: 0.0426\n",
            "Epoch 269/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0027 - val_loss: 0.0428\n",
            "Epoch 270/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0027 - val_loss: 0.0429\n",
            "Epoch 271/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0027 - val_loss: 0.0430\n",
            "Epoch 272/1500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0026 - val_loss: 0.0432\n",
            "Epoch 273/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0026 - val_loss: 0.0434\n",
            "Epoch 274/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0026 - val_loss: 0.0436\n",
            "Epoch 275/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0026 - val_loss: 0.0438\n",
            "Epoch 276/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0026 - val_loss: 0.0439\n",
            "Epoch 277/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0025 - val_loss: 0.0441\n",
            "Epoch 278/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0025 - val_loss: 0.0442\n",
            "Epoch 279/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0025 - val_loss: 0.0445\n",
            "Epoch 280/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0025 - val_loss: 0.0446\n",
            "Epoch 281/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0025 - val_loss: 0.0448\n",
            "Epoch 282/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0024 - val_loss: 0.0450\n",
            "Epoch 283/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0024 - val_loss: 0.0452\n",
            "Epoch 284/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0024 - val_loss: 0.0455\n",
            "Epoch 285/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0024 - val_loss: 0.0457\n",
            "Epoch 286/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0024 - val_loss: 0.0460\n",
            "Epoch 287/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0024 - val_loss: 0.0462\n",
            "Epoch 288/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0023 - val_loss: 0.0464\n",
            "Epoch 289/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0023 - val_loss: 0.0466\n",
            "Epoch 290/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0023 - val_loss: 0.0466\n",
            "Epoch 291/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0023 - val_loss: 0.0466\n",
            "Epoch 292/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0023 - val_loss: 0.0467\n",
            "Epoch 293/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0023 - val_loss: 0.0468\n",
            "Epoch 294/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0023 - val_loss: 0.0469\n",
            "Epoch 295/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0022 - val_loss: 0.0470\n",
            "Epoch 296/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - val_loss: 0.0471\n",
            "Epoch 297/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0022 - val_loss: 0.0472\n",
            "Epoch 298/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0022 - val_loss: 0.0474\n",
            "Epoch 299/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - val_loss: 0.0475\n",
            "Epoch 300/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0022 - val_loss: 0.0477\n",
            "Epoch 301/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - val_loss: 0.0478\n",
            "Epoch 302/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0021 - val_loss: 0.0479\n",
            "Epoch 303/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0021 - val_loss: 0.0481\n",
            "Epoch 304/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0021 - val_loss: 0.0483\n",
            "Epoch 305/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0021 - val_loss: 0.0483\n",
            "Epoch 306/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0021 - val_loss: 0.0484\n",
            "Epoch 307/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0021 - val_loss: 0.0486\n",
            "Epoch 308/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0021 - val_loss: 0.0488\n",
            "Epoch 309/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0021 - val_loss: 0.0489\n",
            "Epoch 310/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0021 - val_loss: 0.0490\n",
            "Epoch 311/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0020 - val_loss: 0.0492\n",
            "Epoch 312/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0020 - val_loss: 0.0494\n",
            "Epoch 313/1500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0020 - val_loss: 0.0495\n",
            "Epoch 314/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0020 - val_loss: 0.0496\n",
            "Epoch 315/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0020 - val_loss: 0.0497\n",
            "Epoch 316/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0020 - val_loss: 0.0498\n",
            "Epoch 317/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0020 - val_loss: 0.0498\n",
            "Epoch 318/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0020 - val_loss: 0.0498\n",
            "Epoch 319/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0019 - val_loss: 0.0500\n",
            "Epoch 320/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 0.0503\n",
            "Epoch 321/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0019 - val_loss: 0.0504\n",
            "Epoch 322/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 0.0503\n",
            "Epoch 323/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 0.0504\n",
            "Epoch 324/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0019 - val_loss: 0.0506\n",
            "Epoch 325/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0019 - val_loss: 0.0507\n",
            "Epoch 326/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0019 - val_loss: 0.0507\n",
            "Epoch 327/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0019 - val_loss: 0.0508\n",
            "Epoch 328/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0019 - val_loss: 0.0511\n",
            "Epoch 329/1500\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0019 - val_loss: 0.0512\n",
            "Epoch 330/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0018 - val_loss: 0.0513\n",
            "Epoch 331/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0018 - val_loss: 0.0514\n",
            "Epoch 332/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0018 - val_loss: 0.0516\n",
            "Epoch 333/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0516\n",
            "Epoch 334/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0018 - val_loss: 0.0516\n",
            "Epoch 335/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0018 - val_loss: 0.0516\n",
            "Epoch 336/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0018 - val_loss: 0.0516\n",
            "Epoch 337/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0018 - val_loss: 0.0513\n",
            "Epoch 338/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0018 - val_loss: 0.0511\n",
            "Epoch 339/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0018 - val_loss: 0.0512\n",
            "Epoch 340/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0017 - val_loss: 0.0513\n",
            "Epoch 341/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0018 - val_loss: 0.0512\n",
            "Epoch 342/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0017 - val_loss: 0.0509\n",
            "Epoch 343/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0017 - val_loss: 0.0509\n",
            "Epoch 344/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0017 - val_loss: 0.0511\n",
            "Epoch 345/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0017 - val_loss: 0.0511\n",
            "Epoch 346/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0017 - val_loss: 0.0508\n",
            "Epoch 347/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0507\n",
            "Epoch 348/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0017 - val_loss: 0.0508\n",
            "Epoch 349/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0510\n",
            "Epoch 350/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0017 - val_loss: 0.0510\n",
            "Epoch 351/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0016 - val_loss: 0.0509\n",
            "Epoch 352/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0016 - val_loss: 0.0509\n",
            "Epoch 353/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0016 - val_loss: 0.0509\n",
            "Epoch 354/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0016 - val_loss: 0.0510\n",
            "Epoch 355/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0512\n",
            "Epoch 356/1500\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0016 - val_loss: 0.0514\n",
            "Epoch 357/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0514\n",
            "Epoch 358/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0016 - val_loss: 0.0511\n",
            "Epoch 359/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0016 - val_loss: 0.0512\n",
            "Epoch 360/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0016 - val_loss: 0.0515\n",
            "Epoch 361/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0016 - val_loss: 0.0517\n",
            "Epoch 362/1500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0016 - val_loss: 0.0513\n",
            "Epoch 363/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0015 - val_loss: 0.0509\n",
            "Epoch 364/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0016 - val_loss: 0.0510\n",
            "Epoch 365/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0015 - val_loss: 0.0513\n",
            "Epoch 366/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0015 - val_loss: 0.0514\n",
            "Epoch 367/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0514\n",
            "Epoch 368/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0513\n",
            "Epoch 369/1500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0015 - val_loss: 0.0514\n",
            "Epoch 370/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0015 - val_loss: 0.0516\n",
            "Epoch 371/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0015 - val_loss: 0.0516\n",
            "Epoch 372/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0015 - val_loss: 0.0515\n",
            "Epoch 373/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0014 - val_loss: 0.0515\n",
            "Epoch 374/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0015 - val_loss: 0.0518\n",
            "Epoch 375/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0014 - val_loss: 0.0521\n",
            "Epoch 376/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0014 - val_loss: 0.0522\n",
            "Epoch 377/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0014 - val_loss: 0.0521\n",
            "Epoch 378/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0520\n",
            "Epoch 379/1500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0014 - val_loss: 0.0519\n",
            "Epoch 380/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0014 - val_loss: 0.0522\n",
            "Epoch 381/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0014 - val_loss: 0.0524\n",
            "Epoch 382/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0524\n",
            "Epoch 383/1500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0014 - val_loss: 0.0522\n",
            "Epoch 384/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0014 - val_loss: 0.0520\n",
            "Epoch 385/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0014 - val_loss: 0.0524\n",
            "Epoch 386/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0013 - val_loss: 0.0526\n",
            "Epoch 387/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0014 - val_loss: 0.0523\n",
            "Epoch 388/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0013 - val_loss: 0.0519\n",
            "Epoch 389/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0519\n",
            "Epoch 390/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0013 - val_loss: 0.0521\n",
            "Epoch 391/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0013 - val_loss: 0.0524\n",
            "Epoch 392/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0013 - val_loss: 0.0525\n",
            "Epoch 393/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0013 - val_loss: 0.0524\n",
            "Epoch 394/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0013 - val_loss: 0.0522\n",
            "Epoch 395/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0013 - val_loss: 0.0522\n",
            "Epoch 396/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0013 - val_loss: 0.0525\n",
            "Epoch 397/1500\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0013 - val_loss: 0.0522\n",
            "Epoch 398/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0013 - val_loss: 0.0517\n",
            "Epoch 399/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0013 - val_loss: 0.0513\n",
            "Epoch 400/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0012 - val_loss: 0.0513\n",
            "Epoch 401/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0012 - val_loss: 0.0515\n",
            "Epoch 402/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0012 - val_loss: 0.0514\n",
            "Epoch 403/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0012 - val_loss: 0.0511\n",
            "Epoch 404/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0012 - val_loss: 0.0508\n",
            "Epoch 405/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0012 - val_loss: 0.0509\n",
            "Epoch 406/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0012 - val_loss: 0.0512\n",
            "Epoch 407/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0012 - val_loss: 0.0510\n",
            "Epoch 408/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0012 - val_loss: 0.0505\n",
            "Epoch 409/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0012 - val_loss: 0.0505\n",
            "Epoch 410/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0012 - val_loss: 0.0509\n",
            "Epoch 411/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0012 - val_loss: 0.0511\n",
            "Epoch 412/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0012 - val_loss: 0.0507\n",
            "Epoch 413/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0012 - val_loss: 0.0502\n",
            "Epoch 414/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0012 - val_loss: 0.0502\n",
            "Epoch 415/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0011 - val_loss: 0.0506\n",
            "Epoch 416/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0012 - val_loss: 0.0508\n",
            "Epoch 417/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0011 - val_loss: 0.0506\n",
            "Epoch 418/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0011 - val_loss: 0.0505\n",
            "Epoch 419/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0011 - val_loss: 0.0507\n",
            "Epoch 420/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0011 - val_loss: 0.0508\n",
            "Epoch 421/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0011 - val_loss: 0.0507\n",
            "Epoch 422/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0011 - val_loss: 0.0507\n",
            "Epoch 423/1500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0011 - val_loss: 0.0508\n",
            "Epoch 424/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0011 - val_loss: 0.0508\n",
            "Epoch 425/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0011 - val_loss: 0.0508\n",
            "Epoch 426/1500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0011 - val_loss: 0.0507\n",
            "Epoch 427/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0011 - val_loss: 0.0507\n",
            "Epoch 428/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0011 - val_loss: 0.0505\n",
            "Epoch 429/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0011 - val_loss: 0.0504\n",
            "Epoch 430/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0010 - val_loss: 0.0506\n",
            "Epoch 431/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0010 - val_loss: 0.0507\n",
            "Epoch 432/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0010 - val_loss: 0.0507\n",
            "Epoch 433/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0010 - val_loss: 0.0506\n",
            "Epoch 434/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0010 - val_loss: 0.0507\n",
            "Epoch 435/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0010 - val_loss: 0.0507\n",
            "Epoch 436/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.0010 - val_loss: 0.0506\n",
            "Epoch 437/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0010 - val_loss: 0.0507\n",
            "Epoch 438/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0010 - val_loss: 0.0509\n",
            "Epoch 439/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 9.9781e-04 - val_loss: 0.0508\n",
            "Epoch 440/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 9.8945e-04 - val_loss: 0.0509\n",
            "Epoch 441/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9.8278e-04 - val_loss: 0.0510\n",
            "Epoch 442/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9.8390e-04 - val_loss: 0.0508\n",
            "Epoch 443/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 9.7296e-04 - val_loss: 0.0506\n",
            "Epoch 444/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 9.7104e-04 - val_loss: 0.0506\n",
            "Epoch 445/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 9.6034e-04 - val_loss: 0.0507\n",
            "Epoch 446/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 9.5473e-04 - val_loss: 0.0506\n",
            "Epoch 447/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 9.4820e-04 - val_loss: 0.0508\n",
            "Epoch 448/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 9.4448e-04 - val_loss: 0.0510\n",
            "Epoch 449/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 9.3907e-04 - val_loss: 0.0510\n",
            "Epoch 450/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 9.3232e-04 - val_loss: 0.0511\n",
            "Epoch 451/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 9.2689e-04 - val_loss: 0.0512\n",
            "Epoch 452/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 9.2148e-04 - val_loss: 0.0510\n",
            "Epoch 453/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 9.1431e-04 - val_loss: 0.0506\n",
            "Epoch 454/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 9.1388e-04 - val_loss: 0.0506\n",
            "Epoch 455/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 9.0307e-04 - val_loss: 0.0506\n",
            "Epoch 456/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8.9948e-04 - val_loss: 0.0504\n",
            "Epoch 457/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 8.9100e-04 - val_loss: 0.0501\n",
            "Epoch 458/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8.9153e-04 - val_loss: 0.0502\n",
            "Epoch 459/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 8.8111e-04 - val_loss: 0.0505\n",
            "Epoch 460/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.8209e-04 - val_loss: 0.0505\n",
            "Epoch 461/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8.7501e-04 - val_loss: 0.0501\n",
            "Epoch 462/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 8.6417e-04 - val_loss: 0.0500\n",
            "Epoch 463/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 8.6169e-04 - val_loss: 0.0503\n",
            "Epoch 464/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.5441e-04 - val_loss: 0.0504\n",
            "Epoch 465/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 8.5155e-04 - val_loss: 0.0502\n",
            "Epoch 466/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.4718e-04 - val_loss: 0.0500\n",
            "Epoch 467/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 8.4464e-04 - val_loss: 0.0502\n",
            "Epoch 468/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.3347e-04 - val_loss: 0.0502\n",
            "Epoch 469/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8.3598e-04 - val_loss: 0.0500\n",
            "Epoch 470/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8.2420e-04 - val_loss: 0.0501\n",
            "Epoch 471/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8.2070e-04 - val_loss: 0.0504\n",
            "Epoch 472/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8.1501e-04 - val_loss: 0.0502\n",
            "Epoch 473/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 8.0884e-04 - val_loss: 0.0500\n",
            "Epoch 474/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8.0369e-04 - val_loss: 0.0501\n",
            "Epoch 475/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 7.9829e-04 - val_loss: 0.0503\n",
            "Epoch 476/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 7.9393e-04 - val_loss: 0.0502\n",
            "Epoch 477/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.8918e-04 - val_loss: 0.0500\n",
            "Epoch 478/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.8805e-04 - val_loss: 0.0502\n",
            "Epoch 479/1500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 7.7924e-04 - val_loss: 0.0505\n",
            "Epoch 480/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 7.7774e-04 - val_loss: 0.0503\n",
            "Epoch 481/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 7.7091e-04 - val_loss: 0.0498\n",
            "Epoch 482/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 7.7091e-04 - val_loss: 0.0499\n",
            "Epoch 483/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 7.6036e-04 - val_loss: 0.0503\n",
            "Epoch 484/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 7.5980e-04 - val_loss: 0.0501\n",
            "Epoch 485/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 7.5407e-04 - val_loss: 0.0497\n",
            "Epoch 486/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 7.5290e-04 - val_loss: 0.0498\n",
            "Epoch 487/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.4490e-04 - val_loss: 0.0503\n",
            "Epoch 488/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 7.4040e-04 - val_loss: 0.0502\n",
            "Epoch 489/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7.3910e-04 - val_loss: 0.0497\n",
            "Epoch 490/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7.3195e-04 - val_loss: 0.0497\n",
            "Epoch 491/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.2721e-04 - val_loss: 0.0502\n",
            "Epoch 492/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.2097e-04 - val_loss: 0.0501\n",
            "Epoch 493/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7.1890e-04 - val_loss: 0.0497\n",
            "Epoch 494/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 7.1108e-04 - val_loss: 0.0496\n",
            "Epoch 495/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 7.0984e-04 - val_loss: 0.0501\n",
            "Epoch 496/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.0005e-04 - val_loss: 0.0501\n",
            "Epoch 497/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.9999e-04 - val_loss: 0.0497\n",
            "Epoch 498/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.9139e-04 - val_loss: 0.0497\n",
            "Epoch 499/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 6.9044e-04 - val_loss: 0.0500\n",
            "Epoch 500/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 6.8180e-04 - val_loss: 0.0501\n",
            "Epoch 501/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 6.8048e-04 - val_loss: 0.0497\n",
            "Epoch 502/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 6.7311e-04 - val_loss: 0.0497\n",
            "Epoch 503/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.7230e-04 - val_loss: 0.0500\n",
            "Epoch 504/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.6637e-04 - val_loss: 0.0501\n",
            "Epoch 505/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 6.6327e-04 - val_loss: 0.0497\n",
            "Epoch 506/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.5614e-04 - val_loss: 0.0496\n",
            "Epoch 507/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6.5396e-04 - val_loss: 0.0498\n",
            "Epoch 508/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 6.4668e-04 - val_loss: 0.0500\n",
            "Epoch 509/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 6.4565e-04 - val_loss: 0.0497\n",
            "Epoch 510/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.3740e-04 - val_loss: 0.0494\n",
            "Epoch 511/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6.3650e-04 - val_loss: 0.0494\n",
            "Epoch 512/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 6.3131e-04 - val_loss: 0.0496\n",
            "Epoch 513/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6.2709e-04 - val_loss: 0.0496\n",
            "Epoch 514/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 6.2424e-04 - val_loss: 0.0495\n",
            "Epoch 515/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.1807e-04 - val_loss: 0.0497\n",
            "Epoch 516/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6.1622e-04 - val_loss: 0.0498\n",
            "Epoch 517/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.0938e-04 - val_loss: 0.0498\n",
            "Epoch 518/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 6.0699e-04 - val_loss: 0.0497\n",
            "Epoch 519/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 6.0222e-04 - val_loss: 0.0497\n",
            "Epoch 520/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 5.9822e-04 - val_loss: 0.0495\n",
            "Epoch 521/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.9639e-04 - val_loss: 0.0497\n",
            "Epoch 522/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.9088e-04 - val_loss: 0.0498\n",
            "Epoch 523/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5.8906e-04 - val_loss: 0.0498\n",
            "Epoch 524/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 5.8287e-04 - val_loss: 0.0497\n",
            "Epoch 525/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5.8003e-04 - val_loss: 0.0497\n",
            "Epoch 526/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.7802e-04 - val_loss: 0.0498\n",
            "Epoch 527/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.7297e-04 - val_loss: 0.0497\n",
            "Epoch 528/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 5.7244e-04 - val_loss: 0.0495\n",
            "Epoch 529/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5.6506e-04 - val_loss: 0.0493\n",
            "Epoch 530/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.6212e-04 - val_loss: 0.0492\n",
            "Epoch 531/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5.6201e-04 - val_loss: 0.0494\n",
            "Epoch 532/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.5655e-04 - val_loss: 0.0494\n",
            "Epoch 533/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5.5622e-04 - val_loss: 0.0493\n",
            "Epoch 534/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5.4869e-04 - val_loss: 0.0494\n",
            "Epoch 535/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.4449e-04 - val_loss: 0.0493\n",
            "Epoch 536/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 5.4678e-04 - val_loss: 0.0492\n",
            "Epoch 537/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.3654e-04 - val_loss: 0.0493\n",
            "Epoch 538/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 5.3753e-04 - val_loss: 0.0495\n",
            "Epoch 539/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 5.3237e-04 - val_loss: 0.0493\n",
            "Epoch 540/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.2513e-04 - val_loss: 0.0491\n",
            "Epoch 541/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 5.3247e-04 - val_loss: 0.0493\n",
            "Epoch 542/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5.1911e-04 - val_loss: 0.0493\n",
            "Epoch 543/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.1966e-04 - val_loss: 0.0491\n",
            "Epoch 544/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.1503e-04 - val_loss: 0.0491\n",
            "Epoch 545/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5.0922e-04 - val_loss: 0.0492\n",
            "Epoch 546/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.1060e-04 - val_loss: 0.0490\n",
            "Epoch 547/1500\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.0250e-04 - val_loss: 0.0492\n",
            "Epoch 548/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.0421e-04 - val_loss: 0.0492\n",
            "Epoch 549/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.0017e-04 - val_loss: 0.0491\n",
            "Epoch 550/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.9378e-04 - val_loss: 0.0492\n",
            "Epoch 551/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4.9537e-04 - val_loss: 0.0492\n",
            "Epoch 552/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.8746e-04 - val_loss: 0.0492\n",
            "Epoch 553/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4.8644e-04 - val_loss: 0.0492\n",
            "Epoch 554/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.8359e-04 - val_loss: 0.0493\n",
            "Epoch 555/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4.7797e-04 - val_loss: 0.0493\n",
            "Epoch 556/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4.7681e-04 - val_loss: 0.0493\n",
            "Epoch 557/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4.7170e-04 - val_loss: 0.0492\n",
            "Epoch 558/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.6946e-04 - val_loss: 0.0491\n",
            "Epoch 559/1500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4.6641e-04 - val_loss: 0.0493\n",
            "Epoch 560/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.6344e-04 - val_loss: 0.0493\n",
            "Epoch 561/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4.6072e-04 - val_loss: 0.0496\n",
            "Epoch 562/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 4.5939e-04 - val_loss: 0.0495\n",
            "Epoch 563/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4.5432e-04 - val_loss: 0.0492\n",
            "Epoch 564/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.5394e-04 - val_loss: 0.0495\n",
            "Epoch 565/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.5490e-04 - val_loss: 0.0494\n",
            "Epoch 566/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 4.4626e-04 - val_loss: 0.0493\n",
            "Epoch 567/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4.4892e-04 - val_loss: 0.0497\n",
            "Epoch 568/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.4422e-04 - val_loss: 0.0496\n",
            "Epoch 569/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.4016e-04 - val_loss: 0.0492\n",
            "Epoch 570/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.3957e-04 - val_loss: 0.0493\n",
            "Epoch 571/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.3241e-04 - val_loss: 0.0495\n",
            "Epoch 572/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4.3394e-04 - val_loss: 0.0494\n",
            "Epoch 573/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.2684e-04 - val_loss: 0.0493\n",
            "Epoch 574/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 4.2699e-04 - val_loss: 0.0498\n",
            "Epoch 575/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.2945e-04 - val_loss: 0.0496\n",
            "Epoch 576/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4.1923e-04 - val_loss: 0.0494\n",
            "Epoch 577/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.2495e-04 - val_loss: 0.0497\n",
            "Epoch 578/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 4.1712e-04 - val_loss: 0.0497\n",
            "Epoch 579/1500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 4.1470e-04 - val_loss: 0.0494\n",
            "Epoch 580/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.1358e-04 - val_loss: 0.0495\n",
            "Epoch 581/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 4.0678e-04 - val_loss: 0.0497\n",
            "Epoch 582/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.0819e-04 - val_loss: 0.0494\n",
            "Epoch 583/1500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.0186e-04 - val_loss: 0.0492\n",
            "Epoch 584/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.0344e-04 - val_loss: 0.0495\n",
            "Epoch 585/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 4.0114e-04 - val_loss: 0.0495\n",
            "Epoch 586/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 3.9551e-04 - val_loss: 0.0493\n",
            "Epoch 587/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3.9711e-04 - val_loss: 0.0494\n",
            "Epoch 588/1500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 3.9052e-04 - val_loss: 0.0493\n",
            "Epoch 589/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3.8999e-04 - val_loss: 0.0490\n",
            "Epoch 590/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3.8681e-04 - val_loss: 0.0491\n",
            "Epoch 591/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3.8394e-04 - val_loss: 0.0493\n",
            "Epoch 592/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 3.8300e-04 - val_loss: 0.0492\n",
            "Epoch 593/1500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 3.7854e-04 - val_loss: 0.0491\n",
            "Epoch 594/1500\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 3.7921e-04 - val_loss: 0.0494\n",
            "Epoch 595/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3.7896e-04 - val_loss: 0.0492\n",
            "Epoch 596/1500\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.7259e-04 - val_loss: 0.0490\n",
            "Epoch 597/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 3.7652e-04 - val_loss: 0.0493\n",
            "Epoch 598/1500\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.7264e-04 - val_loss: 0.0492\n",
            "Epoch 599/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 3.6717e-04 - val_loss: 0.0490\n",
            "Epoch 600/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 3.7110e-04 - val_loss: 0.0493\n",
            "Epoch 601/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 3.6377e-04 - val_loss: 0.0493\n",
            "Epoch 602/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 3.6276e-04 - val_loss: 0.0490\n",
            "Epoch 603/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3.6121e-04 - val_loss: 0.0491\n",
            "Epoch 604/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3.5592e-04 - val_loss: 0.0492\n",
            "Epoch 605/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3.5703e-04 - val_loss: 0.0491\n",
            "Epoch 606/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.5239e-04 - val_loss: 0.0490\n",
            "Epoch 607/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 3.5153e-04 - val_loss: 0.0492\n",
            "Epoch 608/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3.4955e-04 - val_loss: 0.0492\n",
            "Epoch 609/1500\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.4678e-04 - val_loss: 0.0491\n",
            "Epoch 610/1500\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 3.4593e-04 - val_loss: 0.0493\n",
            "Epoch 611/1500\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.4535e-04 - val_loss: 0.0492\n",
            "Epoch 612/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.4128e-04 - val_loss: 0.0491\n",
            "Epoch 613/1500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.4219e-04 - val_loss: 0.0493\n",
            "Epoch 614/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 3.4151e-04 - val_loss: 0.0491\n",
            "Epoch 615/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 3.3579e-04 - val_loss: 0.0490\n",
            "Epoch 616/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.3950e-04 - val_loss: 0.0493\n",
            "Epoch 617/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 3.3752e-04 - val_loss: 0.0492\n",
            "Epoch 618/1500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.3188e-04 - val_loss: 0.0489\n",
            "Epoch 619/1500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 3.3554e-04 - val_loss: 0.0490\n",
            "Epoch 620/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.2806e-04 - val_loss: 0.0491\n",
            "Epoch 621/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.2875e-04 - val_loss: 0.0490\n",
            "Epoch 622/1500\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.2742e-04 - val_loss: 0.0493\n",
            "Epoch 623/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 3.2277e-04 - val_loss: 0.0493\n",
            "Epoch 624/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.2167e-04 - val_loss: 0.0490\n",
            "Epoch 625/1500\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.2086e-04 - val_loss: 0.0490\n",
            "Epoch 626/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 3.1778e-04 - val_loss: 0.0491\n",
            "Epoch 627/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 3.1742e-04 - val_loss: 0.0490\n",
            "Epoch 628/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.1453e-04 - val_loss: 0.0490\n",
            "Epoch 629/1500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 3.1313e-04 - val_loss: 0.0491\n",
            "Epoch 630/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.1211e-04 - val_loss: 0.0491\n",
            "Epoch 631/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.0938e-04 - val_loss: 0.0490\n",
            "Epoch 632/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.0918e-04 - val_loss: 0.0491\n",
            "Epoch 633/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.0898e-04 - val_loss: 0.0490\n",
            "Epoch 634/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 3.0512e-04 - val_loss: 0.0489\n",
            "Epoch 635/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.0513e-04 - val_loss: 0.0490\n",
            "Epoch 636/1500\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.0330e-04 - val_loss: 0.0489\n",
            "Epoch 637/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.0043e-04 - val_loss: 0.0488\n",
            "Epoch 638/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.0153e-04 - val_loss: 0.0490\n",
            "Epoch 639/1500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9923e-04 - val_loss: 0.0490\n",
            "Epoch 640/1500\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 2.9601e-04 - val_loss: 0.0489\n",
            "Epoch 641/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.9731e-04 - val_loss: 0.0490\n",
            "Epoch 642/1500\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 2.9577e-04 - val_loss: 0.0489\n",
            "Epoch 643/1500\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.9149e-04 - val_loss: 0.0487\n",
            "Epoch 644/1500\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 2.9424e-04 - val_loss: 0.0490\n",
            "Epoch 645/1500\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.9306e-04 - val_loss: 0.0490\n",
            "Epoch 646/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.8722e-04 - val_loss: 0.0488\n",
            "Epoch 647/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.9163e-04 - val_loss: 0.0490\n",
            "Epoch 648/1500\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.8943e-04 - val_loss: 0.0489\n",
            "Epoch 649/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.8388e-04 - val_loss: 0.0487\n",
            "Epoch 650/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.8824e-04 - val_loss: 0.0490\n",
            "Epoch 651/1500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.8468e-04 - val_loss: 0.0489\n",
            "Epoch 652/1500\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 2.8041e-04 - val_loss: 0.0487\n",
            "Epoch 653/1500\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 2.8442e-04 - val_loss: 0.0489\n",
            "Epoch 654/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.7979e-04 - val_loss: 0.0489\n",
            "Epoch 655/1500\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 2.7717e-04 - val_loss: 0.0487\n",
            "Epoch 656/1500\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.7961e-04 - val_loss: 0.0490\n",
            "Epoch 657/1500\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.7491e-04 - val_loss: 0.0489\n",
            "Epoch 658/1500\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.7363e-04 - val_loss: 0.0487\n",
            "Epoch 659/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.7467e-04 - val_loss: 0.0488\n",
            "Epoch 660/1500\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.7072e-04 - val_loss: 0.0489\n",
            "Epoch 661/1500\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.7039e-04 - val_loss: 0.0487\n",
            "Epoch 662/1500\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 2.7069e-04 - val_loss: 0.0488\n",
            "Epoch 663/1500\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.6614e-04 - val_loss: 0.0488\n",
            "Epoch 664/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.6738e-04 - val_loss: 0.0487\n",
            "Epoch 665/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.6525e-04 - val_loss: 0.0487\n",
            "Epoch 666/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 2.6374e-04 - val_loss: 0.0488\n",
            "Epoch 667/1500\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.6343e-04 - val_loss: 0.0487\n",
            "Epoch 668/1500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 2.6220e-04 - val_loss: 0.0488\n",
            "Epoch 669/1500\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 2.5951e-04 - val_loss: 0.0488\n",
            "Epoch 670/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.5963e-04 - val_loss: 0.0487\n",
            "Epoch 671/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.5780e-04 - val_loss: 0.0487\n",
            "Epoch 672/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.5688e-04 - val_loss: 0.0487\n",
            "Epoch 673/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.5555e-04 - val_loss: 0.0487\n",
            "Epoch 674/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.5472e-04 - val_loss: 0.0488\n",
            "Epoch 675/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.5440e-04 - val_loss: 0.0487\n",
            "Epoch 676/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.5282e-04 - val_loss: 0.0487\n",
            "Epoch 677/1500\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 2.5185e-04 - val_loss: 0.0487\n",
            "Epoch 678/1500\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.4999e-04 - val_loss: 0.0487\n",
            "Epoch 679/1500\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.4908e-04 - val_loss: 0.0486\n",
            "Epoch 680/1500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.4818e-04 - val_loss: 0.0486\n",
            "Epoch 681/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.4728e-04 - val_loss: 0.0487\n",
            "Epoch 682/1500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.4587e-04 - val_loss: 0.0487\n",
            "Epoch 683/1500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.4496e-04 - val_loss: 0.0487\n",
            "Epoch 684/1500\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.4504e-04 - val_loss: 0.0486\n",
            "Epoch 685/1500\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 2.4432e-04 - val_loss: 0.0488\n",
            "Epoch 686/1500\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.4289e-04 - val_loss: 0.0487\n",
            "Epoch 687/1500\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.4106e-04 - val_loss: 0.0487\n",
            "Epoch 688/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.4135e-04 - val_loss: 0.0487\n",
            "Epoch 689/1500\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 2.4046e-04 - val_loss: 0.0486\n",
            "Epoch 690/1500\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.3877e-04 - val_loss: 0.0486\n",
            "Epoch 691/1500\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 2.3973e-04 - val_loss: 0.0488\n",
            "Epoch 692/1500\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.4095e-04 - val_loss: 0.0487\n",
            "Epoch 693/1500\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.3610e-04 - val_loss: 0.0486\n",
            "Epoch 694/1500\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.3700e-04 - val_loss: 0.0488\n",
            "Epoch 695/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.3914e-04 - val_loss: 0.0486\n",
            "Epoch 696/1500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.3529e-04 - val_loss: 0.0486\n",
            "Epoch 697/1500\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.3335e-04 - val_loss: 0.0487\n",
            "Epoch 698/1500\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.3993e-04 - val_loss: 0.0486\n",
            "Epoch 699/1500\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.3179e-04 - val_loss: 0.0486\n",
            "Epoch 700/1500\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.3346e-04 - val_loss: 0.0488\n",
            "Epoch 701/1500\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 2.3413e-04 - val_loss: 0.0487\n",
            "Epoch 702/1500\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.2984e-04 - val_loss: 0.0486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training and validation loss from history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Grziz6WSvrxb",
        "outputId": "701b62a7-5888-446c-9e3a-a81cac9288a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract training and validation accuracy from history\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(train_acc, label='Train Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Dm8OvJy8bnf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vvL1-MNL42cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)"
      ],
      "metadata": {
        "id": "bWi05-obN-MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced0e34c-3c45-4c74-b83f-3df97848c4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0874\n",
            "Test Loss: 0.08742072433233261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model has been trained and is named 'model'\n",
        "\n",
        "# Predict the next value based on the last window of observations\n",
        "last_window = X_test[-1:]\n",
        "predicted_value_normalized = model.predict(last_window)\n",
        "\n",
        "# Define feature names\n",
        "feature_names = ['Sales', 'Other Revenues', 'Earnings(Pre Tax)', 'Earnings(Post Tax)', 'Crude Oil (Thousand Barrels)', 'Gas (MMcf)', 'Payouts/Dividends', 'Payouts/Dividends Ratio', 'USD to PKR', 'Equity', 'Assets', 'Liabilities', 'Current Assets', 'Non-current Assets', 'Current Liabilities', 'Non Current Liabilities', 'ROE', 'ROA', 'Debt/Equity', 'Debt/Asset', 'SBP Policy Rate']\n",
        "\n",
        "# Scale back the predicted value\n",
        "predicted_value_scaled_back = predicted_value_normalized * features_scale_factor\n",
        "\n",
        "# Convert the predicted value to a scalar (assuming it's a single value)\n",
        "predicted_value_scaled_back_scalar = predicted_value_scaled_back[0]\n",
        "\n",
        "# Create a string with feature name and corresponding scaled back value\n",
        "scaled_back_values_str = ', '.join([f'{name}: {value:.2f}' for name, value in zip(feature_names, predicted_value_scaled_back_scalar)])\n",
        "\n",
        "print(\"Scaled back predicted values:\")\n",
        "print(scaled_back_values_str)\n"
      ],
      "metadata": {
        "id": "7QRGRZbeYj8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df59e479-dce7-4bdc-8e58-fac1852696da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 483ms/step\n",
            "Scaled back predicted values:\n",
            "Sales: 185008625431.66, Other Revenues: 11651210285.43, Earnings(Pre Tax): 108374776904.58, Earnings(Post Tax): 75078820655.94, Crude Oil (Thousand Barrels): 15285.86, Gas (MMcf): 412672.69, Payouts/Dividends: 28588330861.81, Payouts/Dividends Ratio: 0.34, USD to PKR: 133.87, Equity: 181431532985.72, Assets: 415418697697.88, Liabilities: 88778297559.62, Current Assets: 129206494540.31, Non-current Assets: 156896489110.11, Current Liabilities: 35167016853.19, Non Current Liabilities: 69855714203.60, ROE: 0.39, ROA: 0.24, Debt/Equity: 0.45, Debt/Asset: 0.39, SBP Policy Rate: 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model has been trained and is named 'model'\n",
        "\n",
        "# Predict the next value based on the last window of observations\n",
        "last_window = X_test[-1:]\n",
        "predicted_value_normalized = model.predict(last_window)\n",
        "\n",
        "# Define feature names\n",
        "feature_names = ['Sales', 'Other Revenues', 'Earnings(Pre Tax)', 'Earnings(Post Tax)', 'Crude Oil (Thousand Barrels)', 'Gas (MMcf)', 'Payouts/Dividends', 'Payouts/Dividends Ratio', 'USD to PKR', 'Equity', 'Assets', 'Liabilities', 'Current Assets', 'Non-current Assets', 'Current Liabilities', 'Non Current Liabilities', 'ROE', 'ROA', 'Debt/Equity', 'Debt/Asset', 'SBP Policy Rate']\n",
        "\n",
        "# Scale back the predicted value\n",
        "predicted_value_scaled_back = predicted_value_normalized * features_scale_factor\n",
        "\n",
        "# Convert the predicted value to a scalar (assuming it's a single value)\n",
        "predicted_value_scaled_back_scalar = predicted_value_scaled_back[0]\n",
        "\n",
        "# Create a string with feature name and corresponding scaled back value\n",
        "scaled_back_values_str = ', '.join([f'{name}: {value:.2f}' for name, value in zip(feature_names, predicted_value_scaled_back_scalar)])\n",
        "\n",
        "# Iterate over each point in the last window, scale it, and print them individually\n",
        "for i, point in enumerate(last_window[0]):\n",
        "    scaled_point = point * features_scale_factor\n",
        "    point_str = ', '.join([f'{name}: {value:.2f}' for name, value in zip(feature_names, scaled_point)])\n",
        "    print(f\"Point {i+1}:\")\n",
        "    print(point_str)\n",
        "\n",
        "print(\"\\nScaled back predicted values:\")\n",
        "print(scaled_back_values_str)"
      ],
      "metadata": {
        "id": "rzvUII3lplTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2e92a9-60ad-459b-dfe9-c19c38a849a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Point 1:\n",
            "Sales: 261480000000.00, Other Revenues: 37150000000.00, Earnings(Pre Tax): 176600000000.00, Earnings(Post Tax): 118390000000.00, Crude Oil (Thousand Barrels): 14555.00, Gas (MMcf): 370217.00, Payouts/Dividends: 47356000000.00, Payouts/Dividends Ratio: 0.40, USD to PKR: 150.10, Equity: 646214995000.00, Assets: 795058357000.00, Liabilities: 148843362000.00, Current Assets: 509465463000.00, Non-current Assets: 285592894000.00, Current Liabilities: 69902275000.00, Non Current Liabilities: 78941087000.00, ROE: 0.18, ROA: 0.15, Debt/Equity: 0.23, Debt/Asset: 0.19, SBP Policy Rate: 0.13\n",
            "Point 2:\n",
            "Sales: 232930000000.00, Other Revenues: 39880000000.00, Earnings(Pre Tax): 144360000000.00, Earnings(Post Tax): 100940000000.00, Crude Oil (Thousand Barrels): 12919.00, Gas (MMcf): 326879.00, Payouts/Dividends: 29272600000.00, Payouts/Dividends Ratio: 0.29, USD to PKR: 161.13, Equity: 710563976000.00, Assets: 888973636000.00, Liabilities: 178409660000.00, Current Assets: 555071602000.00, Non-current Assets: 333902034000.00, Current Liabilities: 89357746000.00, Non Current Liabilities: 89051914000.00, ROE: 0.14, ROA: 0.11, Debt/Equity: 0.25, Debt/Asset: 0.20, SBP Policy Rate: 0.11\n",
            "Point 3:\n",
            "Sales: 239100000000.00, Other Revenues: 20270000000.00, Earnings(Pre Tax): 128990000000.00, Earnings(Post Tax): 91530000000.00, Crude Oil (Thousand Barrels): 13230.00, Gas (MMcf): 317443.00, Payouts/Dividends: 29289600000.00, Payouts/Dividends Ratio: 0.32, USD to PKR: 162.39, Equity: 769644045000.00, Assets: 955993814000.00, Liabilities: 186349769000.00, Current Assets: 650669655000.00, Non-current Assets: 305324159000.00, Current Liabilities: 101679608000.00, Non Current Liabilities: 84670161000.00, ROE: 0.12, ROA: 0.10, Debt/Equity: 0.24, Debt/Asset: 0.19, SBP Policy Rate: 0.10\n",
            "\n",
            "Scaled back predicted values:\n",
            "Sales: 185008625431.66, Other Revenues: 11651210285.43, Earnings(Pre Tax): 108374776904.58, Earnings(Post Tax): 75078820655.94, Crude Oil (Thousand Barrels): 15285.86, Gas (MMcf): 412672.69, Payouts/Dividends: 28588330861.81, Payouts/Dividends Ratio: 0.34, USD to PKR: 133.87, Equity: 181431532985.72, Assets: 415418697697.88, Liabilities: 88778297559.62, Current Assets: 129206494540.31, Non-current Assets: 156896489110.11, Current Liabilities: 35167016853.19, Non Current Liabilities: 69855714203.60, ROE: 0.39, ROA: 0.24, Debt/Equity: 0.45, Debt/Asset: 0.39, SBP Policy Rate: 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\")\n",
        "print(\"Accounting Error A-L-E in Billions: \")\n",
        "print((predicted_value_scaled_back[0][10]-predicted_value_scaled_back[0][11]-predicted_value_scaled_back[0][9])/1000000000)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Accounting Error A-CA-NCA in Billions: \")\n",
        "print((predicted_value_scaled_back[0][10]-predicted_value_scaled_back[0][12]-predicted_value_scaled_back[0][13])/1000000000)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Accounting Error L-CL-NCL in Billions: \")\n",
        "print((predicted_value_scaled_back[0][11]-predicted_value_scaled_back[0][14]-predicted_value_scaled_back[0][15])/1000000000)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Accounting Error ROE in Ration diff: \")\n",
        "print((predicted_value_scaled_back[0][16]-(predicted_value_scaled_back[0][3]/predicted_value_scaled_back[0][9])))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Accounting Error ROA in Ration diff: \")\n",
        "print((predicted_value_scaled_back[0][17]-(predicted_value_scaled_back[0][3]/predicted_value_scaled_back[0][10])))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Accounting Error Debt/Equity in Ration diff: \")\n",
        "print((predicted_value_scaled_back[0][18]-(predicted_value_scaled_back[0][11]/predicted_value_scaled_back[0][9])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlHATKB6HD-S",
        "outputId": "b6376f41-993d-4b6b-dda0-fd8387177f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accounting Error A-L-E in Billions: \n",
            "145.2088671525418\n",
            "\n",
            "\n",
            "Accounting Error A-CA-NCA in Billions: \n",
            "129.31571404745327\n",
            "\n",
            "\n",
            "Accounting Error L-CL-NCL in Billions: \n",
            "-16.244433497168426\n",
            "\n",
            "\n",
            "Accounting Error ROE in Ration diff: \n",
            "-0.020134199505671024\n",
            "\n",
            "\n",
            "Accounting Error ROA in Ration diff: \n",
            "0.05964155045418121\n",
            "\n",
            "\n",
            "Accounting Error Debt/Equity in Ration diff: \n",
            "-0.041305302094277296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0rPbBiGmI1yO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}